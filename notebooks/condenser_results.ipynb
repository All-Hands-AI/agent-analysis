{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Any, Iterable\n",
    "\n",
    "import altair as alt\n",
    "import pandas as pd\n",
    "\n",
    "from analysis.models.openhands import Evaluation, EvaluationOutput, SWEBenchResult\n",
    "from analysis.usage import total_resource_usage, per_iteration_resource_usage\n",
    "\n",
    "# Altair stores the input data in all visualizations, and we're not being careful about the size of the data we're passing.\n",
    "# If you want to export the visualizations and embed on the web, you might want to comment out this line and look into:\n",
    "# https://altair-viz.github.io/user_guide/large_datasets.html#vegafusion-data-transformer\n",
    "alt.data_transformers.disable_max_rows()\n",
    "\n",
    "# Plug in filepaths to OpenHands evaluation data here -- anything produced using the OpenHands SWE-bench evaluation framework\n",
    "# should be compatible.\n",
    "filepaths = []\n",
    "data = [Evaluation.from_filepath(filepath) for filepath in filepaths]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def classify_error(output: EvaluationOutput, result: SWEBenchResult) -> str:\n",
    "    error = \"other\"\n",
    "    if result.test_result.report.resolved:\n",
    "        error = \"resolved\"\n",
    "    if output.error and not result.test_result.report.resolved:\n",
    "        if output.error.startswith('Agent reached maximum iteration'):\n",
    "                error = 'iter. limit'\n",
    "        if output.error.startswith('Agent got stuck in a loop'):\n",
    "                error = 'event loop'\n",
    "\n",
    "    if error == 'other':\n",
    "        if result.test_result.report.empty_generation:\n",
    "            error = 'empty gen.'\n",
    "        if (\n",
    "            result.test_result.report.error_eval\n",
    "            or result.test_result.report.failed_apply_patch\n",
    "            or result.test_result.report.test_timeout\n",
    "        ):\n",
    "            error = 'test failure'\n",
    "    \n",
    "    return error\n",
    "\n",
    "def per_instance(output: EvaluationOutput, result: SWEBenchResult) -> dict[str, Any]:\n",
    "    return {\n",
    "        'error': classify_error(output, result),\n",
    "        **total_resource_usage(output).model_dump(),\n",
    "        'history_length': len(output.history),\n",
    "        \"finish\": output.history[-1].get(\"action\", \"\") == \"finish\",\n",
    "        **result.test_result.report.model_dump(),\n",
    "    }\n",
    "        \n",
    "\n",
    "df = pd.concat([d.to_dataframe(per_instance) for d in data])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For each experiment, calcuate the ratio of instances resolved.\n",
    "\n",
    "df.copy().groupby('experiment')[\n",
    "    'resolved'\n",
    "].agg(\n",
    "    total_count='size', resolved_count='sum'\n",
    ").assign(\n",
    "    resolution_ratio=lambda x: x['resolved_count'] / x['total_count']\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Not all instances use every iteration allowed by the test harness. Plot the distribution of iterations used.\n",
    "# The color indicates whether the instance was resolved or not.\n",
    "\n",
    "alt.Chart(df).mark_bar().encode(\n",
    "    alt.X('x:Q').title('Iterations'),\n",
    "    alt.Y('count()').title('# of Instances'),\n",
    "    alt.Color('resolved'),\n",
    "    column='experiment',\n",
    ").transform_calculate(x='datum.history_length / 2').properties(width=150, height=150)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Each instance can be resolved or unresolved, and can finish early or late. Too many\n",
    "# instances that finish late (i.e., hit the max iteration limit) might indicate that\n",
    "# the agent needs more iterations to finish the instance.\n",
    "\n",
    "df[\"outcome\"] = df.apply(\n",
    "    lambda row: {\n",
    "        (True, True): \"Resolved Early\",\n",
    "        (True, False): \"Resolved Late\",\n",
    "        (False, True): \"Unresolved Early\",\n",
    "        (False, False): \"Unresolved Late\",\n",
    "    }[(row[\"resolved\"], row[\"finish\"])], axis=1\n",
    ")\n",
    "\n",
    "selection = alt.selection_point(fields=['outcome'])\n",
    "color = (\n",
    "    alt.when(selection)\n",
    "    .then(alt.Color(\"outcome:N\").legend(None))\n",
    "    .otherwise(alt.value(\"lightgray\"))\n",
    ")\n",
    "\n",
    "\n",
    "legend = alt.Chart(df).mark_point().encode(\n",
    "    alt.Y('outcome').axis(orient='right'),\n",
    "    color=color\n",
    ").add_params(\n",
    "    selection\n",
    ")\n",
    "\n",
    "arcs = alt.Chart(df, title=\"Outcomes\").mark_arc().encode(\n",
    "    alt.Theta(\"count()\"), alt.Column(\"experiment\").title(None), color=color\n",
    ").properties(width=150, height=150)\n",
    "\n",
    "arcs | legend"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The main point of condensers is to combat the ever-growing number of tokens per iteration.\n",
    "# We can plot the average number of tokens per iteration for each experiment.\n",
    "\n",
    "def per_step(output: EvaluationOutput, result: SWEBenchResult) -> Iterable[dict[str, Any]]:\n",
    "    for step, step_usage in enumerate(per_iteration_resource_usage(output)):\n",
    "        yield {\n",
    "            \"resolved\": result.test_result.report.resolved,\n",
    "            **step_usage.model_dump(),\n",
    "            \"iteration\": step / 2,\n",
    "        }\n",
    "\n",
    "def post_process(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    df = df.sort_values(by=\"iteration\")\n",
    "    df[\"cumulative_cache_reads\"] = df[\"cache_reads\"].cumsum()\n",
    "    df[\"cumulative_cache_writes\"] = df[\"cache_writes\"].cumsum()\n",
    "    df[\"cache_efficiency\"] = df[\"cumulative_cache_reads\"] - (\n",
    "        0.10 * df[\"cumulative_cache_reads\"] + 1.25 * df[\"cumulative_cache_writes\"]\n",
    "    )\n",
    "    return df\n",
    "\n",
    "df_pi = pd.concat([d.multi_to_dataframe(per_step, post_callback=post_process) for d in data])\n",
    "\n",
    "selection = alt.selection_point(fields=['experiment'])\n",
    "color = (\n",
    "    alt.when(selection)\n",
    "    .then(alt.Color(\"experiment:N\").legend(None))\n",
    "    .otherwise(alt.value(\"lightgray\"))\n",
    ")\n",
    "\n",
    "\n",
    "legend = alt.Chart(df).mark_point().encode(\n",
    "    alt.Y('experiment').axis(orient='right'),\n",
    "    color=color\n",
    ").add_params(\n",
    "    selection\n",
    ")\n",
    "\n",
    "line = (\n",
    "    alt.Chart(df_pi, title=\"Avg. Token Usage per Iteration\")\n",
    "    .mark_line()\n",
    "    .encode(\n",
    "        alt.X(\"iteration\").title(\"Iteration\"),\n",
    "        alt.Y(\"mean(prompt_tokens)\").title(\"Prompt Tokens\"),\n",
    "        color=color,\n",
    "    )\n",
    ")\n",
    "\n",
    "\n",
    "line | legend"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# A secondary consequence of limiting the monotonic growth of tokens is that the LLM\n",
    "# can compute responses faster. We can plot the average response latency per iteration.\n",
    "\n",
    "selection = alt.selection_point(fields=['experiment'])\n",
    "color = (\n",
    "    alt.when(selection)\n",
    "    .then(alt.Color(\"experiment:N\").legend(None))\n",
    "    .otherwise(alt.value(\"lightgray\"))\n",
    ")\n",
    "\n",
    "\n",
    "legend = alt.Chart(df).mark_point().encode(\n",
    "    alt.Y('experiment').axis(orient='right'),\n",
    "    color=color\n",
    ").add_params(\n",
    "    selection\n",
    ")\n",
    "\n",
    "line = (\n",
    "    alt.Chart(df_pi, title=\"Avg. Response Latency per Iteration\")\n",
    "    .mark_line()\n",
    "    .encode(\n",
    "        alt.X(\"iteration\").title(\"Iteration\"),\n",
    "        alt.Y(\"mean(response_latency)\").title(\"Response Latency\"),\n",
    "        color=color,\n",
    "    )\n",
    ")\n",
    "\n",
    "\n",
    "line | legend"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Condensers often act by manipulating the event history sent to the LLM. This has the\n",
    "# downside of invaldiating the LLM's cache, which can lead to more cache reads and writes.\n",
    "\n",
    "# This graph uses Anthropic's cache costs to determine if condensers thrash the event\n",
    "# stream enough to make the LLM's cache a detriment -- anything above the x-axis is able\n",
    "# to utilize the cache, and the further above the line, the more efficient the cache is.\n",
    "\n",
    "line = (\n",
    "    alt.Chart(df_pi)\n",
    "    .mark_line()\n",
    "    .encode(\n",
    "        alt.X('iteration').title('Iteration'),\n",
    "        alt.Y('mean(cache_efficiency):Q').title('Reads - (0.1 * Reads + 1.25 * Writes)'),\n",
    "        alt.Color('experiment').title('Experiment'),\n",
    "    )\n",
    ")\n",
    "\n",
    "band = (\n",
    "    alt.Chart(df_pi, title='Average Cache Cost Savings per Iteration')\n",
    "    .mark_errorband(extent='ci')\n",
    "    .encode(\n",
    "        alt.X('iteration').title('Iteration'),\n",
    "        alt.Y('cache_efficiency:Q').title('Reads - (0.1 * Reads + 1.25 * Writes)'),\n",
    "        alt.Color('experiment').title('Experiment'),\n",
    "    )\n",
    ")\n",
    "\n",
    "band + line"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To better understand how condensers impact something like SWE-bench performance, we can\n",
    "# treat tokens, iterations, and response latency as resources and compute how many\n",
    "# instances we could solve _in parallel_ with a certain amount of each resource.\n",
    "\n",
    "# This is a cactus plot: each line represents a different experiment, the y-axis tracks\n",
    "# the number of instances resolved, and the x-axis tracks the amount of each resource,\n",
    "# and the line indicates how many instances are resolved using that amount of resources\n",
    "# or fewer.\n",
    "\n",
    "def per_instance(output: EvaluationOutput, result: SWEBenchResult) -> dict[str, Any]:\n",
    "        return {\n",
    "            'resolved': 1 if result.test_result.report.resolved else 0,\n",
    "            'iteration': len(output.history) // 2,\n",
    "            **total_resource_usage(output).model_dump(),\n",
    "        }\n",
    "        \n",
    "\n",
    "def cactus_plot_post_process(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    df['total_tokens'] = df['prompt_tokens'] + df['completion_tokens']\n",
    "\n",
    "    for field in (\n",
    "        'iteration',\n",
    "        'prompt_tokens',\n",
    "        'completion_tokens',\n",
    "        'total_tokens',\n",
    "        'response_latency',\n",
    "    ):\n",
    "        df = df.sort_values(by=field)\n",
    "        df[f'resolved_by_{field}'] = df['resolved'].cumsum()\n",
    "\n",
    "    return df\n",
    "\n",
    "df = pd.concat([cactus_plot_post_process(d.to_dataframe(per_instance)) for d in data])\n",
    "\n",
    "selection = alt.selection_point(fields=['experiment'])\n",
    "color = (\n",
    "    alt.when(selection)\n",
    "    .then(alt.Color(\"experiment:N\").legend(None))\n",
    "    .otherwise(alt.value(\"lightgray\"))\n",
    ")\n",
    "\n",
    "\n",
    "legend = alt.Chart(df).mark_point().encode(\n",
    "    alt.Y('experiment').axis(orient='right'),\n",
    "    color=color\n",
    ").add_params(\n",
    "    selection\n",
    ")\n",
    "\n",
    "iter = alt.Chart(df, title='Cumulative Resolved by Iteration').mark_line().encode(\n",
    "    alt.X('iteration:Q').title('Iteration'),\n",
    "    # alt.Y('mean(resolved_by_iteration):Q').title('Cumulative Resolved').scale(domain=(0, 25)),\n",
    "    alt.Y('mean(resolved_by_iteration):Q').title('Cumulative Resolved'),\n",
    "    color=color,\n",
    ").properties(width=150, height=250)\n",
    "\n",
    "token = alt.Chart(df, title='Cumulative Resolved by Token Consumption').mark_line().encode(\n",
    "    alt.X('total_tokens:Q').title('Total Tokens'),\n",
    "    # alt.Y('mean(resolved_by_total_tokens):Q').title(None).scale(domain=(0, 25)),\n",
    "    alt.Y('mean(resolved_by_total_tokens):Q').title(None),\n",
    "    color=color,\n",
    ").properties(width=150, height=250)\n",
    "\n",
    "latency = alt.Chart(df, title='Cumulative Resolved by Latency').mark_line().encode(\n",
    "    alt.X('response_latency:Q').title('Response Latency'),\n",
    "    # alt.Y('mean(resolved_by_response_latency):Q').title(None).scale(domain=(0, 25)),\n",
    "    alt.Y('mean(resolved_by_response_latency):Q').title(None),\n",
    "    color=color,\n",
    ").properties(width=150, height=250)\n",
    "\n",
    "iter | token | latency | legend"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# A useful spot check is to look at patterns in the event history. This graph plots each instance\n",
    "# as a row of circles representing the steps taken by the agent. The color corresponds to the type\n",
    "# of action taken by the agent, and the size of the circle corresponds to the size of the message\n",
    "# sent to the LLM.\n",
    "\n",
    "def per_step(output: EvaluationOutput, result: SWEBenchResult) -> Iterable[dict[str, Any]]:\n",
    "    for i, step in enumerate(output.history):\n",
    "        action = step.get(\"action\", \"observation\")\n",
    "\n",
    "        if action == \"observation\":\n",
    "            message = step[\"content\"]\n",
    "        elif action in (\"run\", \"run_ipython\"):\n",
    "            message = \":\".join(step[\"message\"].split(\":\")[1:])\n",
    "        else:\n",
    "            message = step[\"message\"]\n",
    "\n",
    "        yield {\n",
    "            \"action\": action,\n",
    "            # 'message': message[:100] + \"...\",\n",
    "            \"size\": len(message),\n",
    "            \"step\": i\n",
    "        }\n",
    "\n",
    "\n",
    "\n",
    "alt.Chart(pd.concat([d.multi_to_dataframe(per_step) for d in data])).mark_circle(opacity=1).encode(\n",
    "    alt.X(\"step:O\", title=\"Step\").title(None).axis(None),\n",
    "    alt.Y(\"instance_id\").title(None).axis(None),\n",
    "    alt.Color(\"action:N\", title=\"Action\").legend(None),\n",
    "    alt.Size(\"size:Q\", title=\"Message Size\").scale(type=\"sqrt\").legend(None),\n",
    "    tooltip=[alt.Tooltip(\"size:Q\"), alt.Tooltip(\"action:N\"),],\n",
    "    row=\"experiment\",\n",
    ").properties(width=1000, height=500)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
